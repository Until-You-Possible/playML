### 关于在线性回归中的梯度下降法

我们说在线性回归中，我们有目标函数

![img.png](img/linearGradient/img.png)

使得目标函数尽可能的小。

对于 ŷ 我们在多元线性回归中知道

![img_1.png](img/linearGradient/img_1.png)

带入公式中可以得到：

![img_2.png](img/linearGradient/img_2.png)

那么这个函数其实 就是我们的 损失函数(loss function)

假设定义我们损失函数 ΔJ(θ), 对其θ进行求导，可以得到：

![img_3.png](img/linearGradient/img_3.png)，我们可以得到这样的向量

中间推导公式省略。。。

最终得到目标函数  

![img_4.png](img/linearGradient/img_4.png)
